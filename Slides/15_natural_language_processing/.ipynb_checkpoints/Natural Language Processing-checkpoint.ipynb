{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "import random\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import nltk\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "*Adapted from the online nltk book [Natural Language Processing with Python](http://www.nltk.org/book) as well as examples from Rutu Mehta's [Pydata talk](https://www.youtube.com/watch?v=gJwFHSeFg44)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK has some really great tools for searching and understanding large bodies of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 11 of 11 matches:\n",
      "ong the former , one was of a most monstrous size . ... This came towards us , \n",
      "ON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r\n",
      "ll over with a heathenish array of monstrous clubs and spears . Some were thick\n",
      "d as you gazed , and wondered what monstrous cannibal and savage could ever hav\n",
      "that has survived the flood ; most monstrous and most mountainous ! That Himmal\n",
      "they might scout at Moby Dick as a monstrous fable , or still worse and more de\n",
      "th of Radney .'\" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l\n",
      "ing Scenes . In connexion with the monstrous pictures of whales , I am strongly\n",
      "ere to enter upon those still more monstrous stories of them which are to be fo\n",
      "ght have been rummaged out of this monstrous cabinet there is no telling . But \n",
      "of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u\n"
     ]
    }
   ],
   "source": [
    "text1.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imperial subtly impalpable pitiable curious abundant perilous\n",
      "trustworthy untoward singular lamentable few determined maddens\n",
      "horrible tyrannical lazy mystifying christian exasperate\n"
     ]
    }
   ],
   "source": [
    "text1.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "very exceedingly so heartily a great good amazingly as sweet\n",
      "remarkably extremely vast\n"
     ]
    }
   ],
   "source": [
    "text2.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look and plot the relative frequency of different words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist1 = FreqDist(text7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u',', 4885),\n",
       " (u'the', 4045),\n",
       " (u'.', 3828),\n",
       " (u'of', 2319),\n",
       " (u'to', 2164),\n",
       " (u'a', 1878),\n",
       " (u'in', 1572),\n",
       " (u'and', 1511),\n",
       " (u'*-1', 1123),\n",
       " (u'0', 1099)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist1.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the most common words that are over 15 letters long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'responsibilities', 27)\n",
      "(u'instrumentalities', 6)\n",
      "(u'misunderstanding', 5)\n",
      "(u'constitutionally', 4)\n"
     ]
    }
   ],
   "source": [
    "fdist1 = FreqDist(text4)\n",
    "for x in fdist1.most_common(10000):\n",
    "    if len(x[0]) > 15 and x[1] > 3:\n",
    "        print x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also quickly find collocations (words that appear close to each other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "million *U*; New York; billion *U*; Wall Street; program trading; Mrs.\n",
      "Yeargin; vice president; Stock Exchange; Big Board; Georgia Gulf;\n",
      "chief executive; Dow Jones; S&P 500; says *T*-1; York Stock; last\n",
      "year; Sea Containers; South Korea; American Express; San Francisco\n"
     ]
    }
   ],
   "source": [
    "text7.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordnet is a collection of words often used in spellcheck. We can use wordnet to do some interesting things:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word Puzzle](WordPuzzle.png)\n",
    "Word Puzzle easily solved with nltk: Which words use at most one of the white letters, and definitely contain the center letter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glover\n",
      "gorlin\n",
      "govern\n",
      "grovel\n",
      "ignore\n",
      "involver\n",
      "lienor\n",
      "linger\n",
      "longer\n",
      "lovering\n",
      "noiler\n",
      "overling\n",
      "region\n",
      "renvoi\n",
      "ringle\n",
      "roving\n",
      "violer\n",
      "virole\n"
     ]
    }
   ],
   "source": [
    "fq = FreqDist('egivrvonl')\n",
    "for w in nltk.corpus.words.words():\n",
    "    if 'r' in w:\n",
    "        if FreqDist(w) < fq:\n",
    "            if len(w) >= 6:\n",
    "                print w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We know nltk can do a lot. But we use sklearn. How can we combine the two? How can we add a lemmatizer to sklearn?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's understand a tokenizer in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = [\"He either was or wasn't really excited about the baseball games\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'either',\n",
       " 'was',\n",
       " 'or',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'really',\n",
       " 'excited',\n",
       " 'about',\n",
       " 'the',\n",
       " 'baseball',\n",
       " 'games']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(s[0])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how about the Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'either',\n",
       " u'wa',\n",
       " 'or',\n",
       " u'wa',\n",
       " \"n't\",\n",
       " 'really',\n",
       " 'excited',\n",
       " 'about',\n",
       " 'the',\n",
       " 'baseball',\n",
       " u'game']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "[wnl.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder, we use CountVectorizer() to convert words into numbers. Each word becomes a column in the new matrix. What if we want to use our own tokenizer. How can we do so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CountVectorizer??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Intro to Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class objects support two types of operations: attribute reference, and instatiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GAClass:\n",
    "    num_students=10\n",
    "    def announcement(self):\n",
    "        print \"There are %s in this class\" % self.num_students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute reference: 10\n",
      "There are 10 in this class\n"
     ]
    }
   ],
   "source": [
    "g = GAClass()\n",
    "print \"Attribute reference: \" + str(g.num_students)\n",
    "g.announcement()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why does it seem that announcement takes an argument, but we never passed any?\n",
    "\n",
    "By default, when you call a class method, it passes in the object as the first parameter.\n",
    "\n",
    "When calling a class method, by default it passes in the Object as the first parameter. Calling g.announcement() is the same as calling GAClass.announcment(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 in this class\n"
     ]
    }
   ],
   "source": [
    "GAClass.announcement(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might sometimes want to create an object with certain properties, right from the beginning. In this case, you would want to create a special method called `__init__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GAClass:\n",
    "    def __init__(self,n_students):\n",
    "        self.num_students = n_students\n",
    "    def announcement(self):\n",
    "        print \"There are %s in this class\" % self.num_students\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15 in this class\n"
     ]
    }
   ],
   "source": [
    "g = GAClass(15)\n",
    "g.announcement()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another special method called `__call__`. This let's you do things like g(10). Perhaps you wanted that to mean square the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GAClass:\n",
    "    def __init__(self,n_students):\n",
    "        self.num_students = n_students\n",
    "    def __call__(self, some_num):\n",
    "        return some_num**2\n",
    "    def announcement(self):\n",
    "        print \"There are %s in this class\" % self.num_students\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = GAClass(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#you can pass in a tokenizer/Lemmatizer into CountVectorizer, but it\n",
    "#has to be a callable object.\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "         self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(tokenizer=LemmaTokenizer())  \n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = [\"He was or wasn't really excited about the baseball games\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect.fit(s)\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv.fit(s)\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's see if we can use some basic NLP features to extract the most popular topics during the Seatle pydata talks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pprint??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '../../data/pydata_talks.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a1a26953c462>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/pydata_talks.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '../../data/pydata_talks.csv'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data = []\n",
    "with open('../../data/pydata_talks.csv') as fin:\n",
    "    for line in fin:\n",
    "        data.append(json.loads(line))\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>description</th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Registration and Breakfast</td>\n",
       "      <td>Registration and Breakfast</td>\n",
       "      <td>Friday 8 a.m.-9 a.m.</td>\n",
       "      <td>Registration and Breakfast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>See description.</td>\n",
       "      <td>Software and computers are everywhere, revolut...</td>\n",
       "      <td>Friday 9 a.m.-9:50 a.m.</td>\n",
       "      <td>Keynote: Computer Science: America's Untapped ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Machine learning is the branch of computer sci...</td>\n",
       "      <td>This tutorial will offer an introduction to th...</td>\n",
       "      <td>Friday 10 a.m.-noon</td>\n",
       "      <td>Machine Learning with Scikit-Learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The first part of the tutorial will cover basi...</td>\n",
       "      <td>The goal of this tutorial is to provide effici...</td>\n",
       "      <td>Friday 10 a.m.-noon</td>\n",
       "      <td>Python for Data Science: A Rapid On-ramp Primer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When I started learning more about statistics ...</td>\n",
       "      <td>We will learn how to make valid statistical in...</td>\n",
       "      <td>Friday 10 a.m.-noon</td>\n",
       "      <td>Simplified statistics through simulation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0                         Registration and Breakfast   \n",
       "1                                  See description.    \n",
       "2  Machine learning is the branch of computer sci...   \n",
       "3  The first part of the tutorial will cover basi...   \n",
       "4  When I started learning more about statistics ...   \n",
       "\n",
       "                                         description                     time  \\\n",
       "0                         Registration and Breakfast     Friday 8 a.m.-9 a.m.   \n",
       "1  Software and computers are everywhere, revolut...  Friday 9 a.m.-9:50 a.m.   \n",
       "2  This tutorial will offer an introduction to th...      Friday 10 a.m.-noon   \n",
       "3  The goal of this tutorial is to provide effici...      Friday 10 a.m.-noon   \n",
       "4  We will learn how to make valid statistical in...      Friday 10 a.m.-noon   \n",
       "\n",
       "                                               title  \n",
       "0                         Registration and Breakfast  \n",
       "1  Keynote: Computer Science: America's Untapped ...  \n",
       "2                 Machine Learning with Scikit-Learn  \n",
       "3    Python for Data Science: A Rapid On-ramp Primer  \n",
       "4           Simplified statistics through simulation  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e2ad3a76b526>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all_data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstract\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['all_data'] = df.abstract + \" \" + df.description + \" \" + df.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets try a basic CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(df.all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'the', u'and', u'of', u'to', u'in', u'data', u'for', u'is', u'with', u'this']\n"
     ]
    }
   ],
   "source": [
    "fn = np.array(cv.get_feature_names())\n",
    "common_words = fn[np.argsort(X.toarray().sum(axis=0))[-10:]]\n",
    "pprint(list(common_words[::-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw there was limited success there. We should enhance it by removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "X = cv.fit_transform(df.all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'data',\n",
      " u'python',\n",
      " u'learning',\n",
      " u'talk',\n",
      " u'using',\n",
      " u'analysis',\n",
      " u'll',\n",
      " u'use',\n",
      " u'pandas',\n",
      " u'machine',\n",
      " u'models',\n",
      " u'tools',\n",
      " u'time',\n",
      " u'analytics',\n",
      " u'code',\n",
      " u'science',\n",
      " u'based',\n",
      " u'discuss',\n",
      " u'spark',\n",
      " u'model']\n"
     ]
    }
   ],
   "source": [
    "fn = np.array(cv.get_feature_names())\n",
    "common_words = fn[np.argsort(X.toarray().sum(axis=0))[-20:]]\n",
    "pprint(list(common_words[::-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we can try weighing words inversly by their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = TfidfVectorizer(stop_words='english')\n",
    "X = cv.fit_transform(df.all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'data',\n",
      " u'soon',\n",
      " u'coming',\n",
      " u'python',\n",
      " u'lunch',\n",
      " u'learning',\n",
      " u'talk',\n",
      " u'breakfast',\n",
      " u'using',\n",
      " u'break',\n",
      " u'snacks',\n",
      " u'analysis',\n",
      " u'll',\n",
      " u'pandas',\n",
      " u'use',\n",
      " u'machine',\n",
      " u'spark',\n",
      " u'science',\n",
      " u'code',\n",
      " u'registration']\n"
     ]
    }
   ],
   "source": [
    "fn = np.array(cv.get_feature_names())\n",
    "common_words = fn[np.argsort(X.toarray().sum(axis=0))[-20:]]\n",
    "pprint(list(common_words[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'talk', 'NN'),\n",
       " (u'analysis', 'NN'),\n",
       " (u'use', 'NN'),\n",
       " (u'machine', 'NN'),\n",
       " (u'science', 'NN'),\n",
       " (u'model', 'NN'),\n",
       " (u'time', 'NN'),\n",
       " (u'work', 'NN'),\n",
       " (u'software', 'NN'),\n",
       " (u'performance', 'NN'),\n",
       " (u'space', 'NN'),\n",
       " (u'deep', 'NN')]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter(lambda x: x[1] == 'NN', pos_tag(common_words[::-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our own preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "class my_preprocessor(object):\n",
    "    def __init__(self):\n",
    "        self.spchars = re.compile('\\`|\\~|\\!|\\@|\\#|\\$|\\%|\\^|\\&|\\*|\\(|\\)|\\_|\\+|\\=|\\\\|\\||\\{|\\[|\\]|\\}|\\:|\\;|\\'|\\\"|\\<|\\,|\\>|\\?|\\/|\\.|\\-')\n",
    "    def __call__(self, doc):\n",
    "        return self.spchars.sub('', doc.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(preprocessor=my_preprocessor(),stop_words='english')\n",
    "X = cv.fit_transform(df.all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'data',\n",
      " u'python',\n",
      " u'learning',\n",
      " u'talk',\n",
      " u'using',\n",
      " u'analysis',\n",
      " u'use',\n",
      " u'pandas',\n",
      " u'models',\n",
      " u'machine',\n",
      " u'tools',\n",
      " u'ill',\n",
      " u'analytics',\n",
      " u'code',\n",
      " u'science',\n",
      " u'discuss',\n",
      " u'new',\n",
      " u'model',\n",
      " u'like',\n",
      " u'used',\n",
      " u'tutorial',\n",
      " u'time',\n",
      " u'techniques',\n",
      " u'spark',\n",
      " u'work',\n",
      " u'library',\n",
      " u'methods',\n",
      " u'coming',\n",
      " u'software',\n",
      " u'based',\n",
      " u'algorithm',\n",
      " u'algorithms',\n",
      " u'building',\n",
      " u'soon',\n",
      " u'libraries',\n",
      " u'applications',\n",
      " u'statistical',\n",
      " u'performance',\n",
      " u'space',\n",
      " u'deep']\n"
     ]
    }
   ],
   "source": [
    "fn = np.array(cv.get_feature_names())\n",
    "common_words = fn[np.argsort(X.toarray().sum(axis=0))[-40:]]\n",
    "pprint(list(common_words[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'talk', 'NN'),\n",
       " (u'analysis', 'NN'),\n",
       " (u'use', 'NN'),\n",
       " (u'machine', 'NN'),\n",
       " (u'science', 'NN'),\n",
       " (u'model', 'NN'),\n",
       " (u'time', 'NN'),\n",
       " (u'work', 'NN'),\n",
       " (u'software', 'NN'),\n",
       " (u'performance', 'NN'),\n",
       " (u'space', 'NN'),\n",
       " (u'deep', 'NN')]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter(lambda x: x[1] == 'NN', pos_tag(common_words[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
