{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "import random\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import nltk\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "*Adapted from the online nltk book [Natural Language Processing with Python](http://www.nltk.org/book) as well as examples from Rutu Mehta's [Pydata talk](https://www.youtube.com/watch?v=gJwFHSeFg44)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK has some really great tools for searching and understanding large bodies of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text1.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text1.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text2.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look and plot the relative frequency of different words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist1 = FreqDist(text7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist1.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the most common words that are over 15 letters long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist1 = FreqDist(text4)\n",
    "for x in fdist1.most_common(10000):\n",
    "    if len(x[0]) > 15 and x[1] > 3:\n",
    "        print x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also quickly find collocations (words that appear close to each other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text7.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordnet is a collection of words often used in spellcheck. We can use wordnet to do some interesting things:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word Puzzle](WordPuzzle.png)\n",
    "Word Puzzle easily solved with nltk: Which words use at most one of the white letters, and definitely contain the center letter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fq = FreqDist('egivrvonl')\n",
    "for w in nltk.corpus.words.words():\n",
    "    if 'r' in w:\n",
    "        if FreqDist(w) < fq:\n",
    "            if len(w) >= 6:\n",
    "                print w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We know nltk can do a lot. But we use sklearn. How can we combine the two? How can we add a lemmatizer to sklearn?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's understand a tokenizer in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = [\"He either was or wasn't really excited about the baseball games\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'either',\n",
       " 'was',\n",
       " 'or',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'really',\n",
       " 'excited',\n",
       " 'about',\n",
       " 'the',\n",
       " 'baseball',\n",
       " 'games']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(s[0])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how about the Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'either',\n",
       " u'wa',\n",
       " 'or',\n",
       " u'wa',\n",
       " \"n't\",\n",
       " 'really',\n",
       " 'excited',\n",
       " 'about',\n",
       " 'the',\n",
       " 'baseball',\n",
       " u'game']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "[wnl.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder, we use CountVectorizer() to convert words into numbers. Each word becomes a column in the new matrix. What if we want to use our own tokenizer. How can we do so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CountVectorizer??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Intro to Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class objects support two types of operations: attribute reference, and instatiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GAClass:\n",
    "    num_students=10\n",
    "    def announcement(self):\n",
    "        print \"There are %s in this class\" % self.num_students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute reference: 10\n",
      "There are 10 in this class\n"
     ]
    }
   ],
   "source": [
    "g = GAClass()\n",
    "print \"Attribute reference: \" + str(g.num_students)\n",
    "g.announcement()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why does it seem that announcement takes an argument, but we never passed any?\n",
    "\n",
    "By default, when you call a class method, it passes in the object as the first parameter.\n",
    "\n",
    "When calling a class method, by default it passes in the Object as the first parameter. Calling g.announcement() is the same as calling GAClass.announcment(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 in this class\n"
     ]
    }
   ],
   "source": [
    "GAClass.announcement(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might sometimes want to create an object with certain properties, right from the beginning. In this case, you would want to create a special method called `__init__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GAClass:\n",
    "    def __init__(self,n_students):\n",
    "        self.num_students = n_students\n",
    "    def announcement(self):\n",
    "        print \"There are %s in this class\" % self.num_students\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15 in this class\n"
     ]
    }
   ],
   "source": [
    "g = GAClass(15)\n",
    "g.announcement()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another special method called `__call__`. This let's you do things like g(10). Perhaps you wanted that to mean square the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GAClass:\n",
    "    def __init__(self,n_students):\n",
    "        self.num_students = n_students\n",
    "    def __call__(self, some_num):\n",
    "        return some_num**2\n",
    "    def announcement(self):\n",
    "        print \"There are %s in this class\" % self.num_students\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = GAClass(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#you can pass in a tokenizer/Lemmatizer into CountVectorizer, but it\n",
    "#has to be a callable object.\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "         self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(tokenizer=LemmaTokenizer())  \n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = [\"He was or wasn't really excited about the baseball games\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect.fit(s)\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv.fit(s)\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's see if we can use some basic NLP features to extract the most popular topics during the Seatle pydata talks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data = []\n",
    "with open('../../data/pydata_talks.csv') as fin:\n",
    "    for line in fin:\n",
    "        data.append(json.loads(line))\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>description</th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Registration and Breakfast</td>\n",
       "      <td>Registration and Breakfast</td>\n",
       "      <td>Friday 8 a.m.-9 a.m.</td>\n",
       "      <td>Registration and Breakfast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>See description.</td>\n",
       "      <td>Software and computers are everywhere, revolut...</td>\n",
       "      <td>Friday 9 a.m.-9:50 a.m.</td>\n",
       "      <td>Keynote: Computer Science: America's Untapped ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Machine learning is the branch of computer sci...</td>\n",
       "      <td>This tutorial will offer an introduction to th...</td>\n",
       "      <td>Friday 10 a.m.-noon</td>\n",
       "      <td>Machine Learning with Scikit-Learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The first part of the tutorial will cover basi...</td>\n",
       "      <td>The goal of this tutorial is to provide effici...</td>\n",
       "      <td>Friday 10 a.m.-noon</td>\n",
       "      <td>Python for Data Science: A Rapid On-ramp Primer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When I started learning more about statistics ...</td>\n",
       "      <td>We will learn how to make valid statistical in...</td>\n",
       "      <td>Friday 10 a.m.-noon</td>\n",
       "      <td>Simplified statistics through simulation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0                         Registration and Breakfast   \n",
       "1                                  See description.    \n",
       "2  Machine learning is the branch of computer sci...   \n",
       "3  The first part of the tutorial will cover basi...   \n",
       "4  When I started learning more about statistics ...   \n",
       "\n",
       "                                         description                     time  \\\n",
       "0                         Registration and Breakfast     Friday 8 a.m.-9 a.m.   \n",
       "1  Software and computers are everywhere, revolut...  Friday 9 a.m.-9:50 a.m.   \n",
       "2  This tutorial will offer an introduction to th...      Friday 10 a.m.-noon   \n",
       "3  The goal of this tutorial is to provide effici...      Friday 10 a.m.-noon   \n",
       "4  We will learn how to make valid statistical in...      Friday 10 a.m.-noon   \n",
       "\n",
       "                                               title  \n",
       "0                         Registration and Breakfast  \n",
       "1  Keynote: Computer Science: America's Untapped ...  \n",
       "2                 Machine Learning with Scikit-Learn  \n",
       "3    Python for Data Science: A Rapid On-ramp Primer  \n",
       "4           Simplified statistics through simulation  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['all_data'] = df.abstract + \" \" + df.description + \" \" + df.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets try a basic CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(df.all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'the', u'and', u'of', u'to', u'in', u'data', u'for', u'is', u'with', u'this']\n"
     ]
    }
   ],
   "source": [
    "fn = np.array(cv.get_feature_names())\n",
    "common_words = fn[np.argsort(X.toarray().sum(axis=0))[-10:]]\n",
    "pprint(list(common_words[::-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw there was limited success there. We should enhance it by removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "X = cv.fit_transform(df.all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'data',\n",
      " u'python',\n",
      " u'learning',\n",
      " u'talk',\n",
      " u'using',\n",
      " u'analysis',\n",
      " u'll',\n",
      " u'use',\n",
      " u'pandas',\n",
      " u'machine',\n",
      " u'models',\n",
      " u'tools',\n",
      " u'time',\n",
      " u'analytics',\n",
      " u'code',\n",
      " u'science',\n",
      " u'based',\n",
      " u'discuss',\n",
      " u'spark',\n",
      " u'model']\n"
     ]
    }
   ],
   "source": [
    "fn = np.array(cv.get_feature_names())\n",
    "common_words = fn[np.argsort(X.toarray().sum(axis=0))[-20:]]\n",
    "pprint(list(common_words[::-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we can try weighing words inversly by their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = TfidfVectorizer(stop_words='english')\n",
    "X = cv.fit_transform(df.all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'data',\n",
      " u'soon',\n",
      " u'coming',\n",
      " u'python',\n",
      " u'lunch',\n",
      " u'learning',\n",
      " u'talk',\n",
      " u'breakfast',\n",
      " u'using',\n",
      " u'break',\n",
      " u'snacks',\n",
      " u'analysis',\n",
      " u'll',\n",
      " u'pandas',\n",
      " u'use',\n",
      " u'machine',\n",
      " u'spark',\n",
      " u'science',\n",
      " u'code',\n",
      " u'registration']\n"
     ]
    }
   ],
   "source": [
    "fn = np.array(cv.get_feature_names())\n",
    "common_words = fn[np.argsort(X.toarray().sum(axis=0))[-20:]]\n",
    "pprint(list(common_words[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'talk', 'NN'),\n",
       " (u'analysis', 'NN'),\n",
       " (u'use', 'NN'),\n",
       " (u'machine', 'NN'),\n",
       " (u'science', 'NN'),\n",
       " (u'model', 'NN'),\n",
       " (u'time', 'NN'),\n",
       " (u'work', 'NN'),\n",
       " (u'software', 'NN'),\n",
       " (u'performance', 'NN'),\n",
       " (u'space', 'NN'),\n",
       " (u'deep', 'NN')]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter(lambda x: x[1] == 'NN', pos_tag(common_words[::-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our own preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "class my_preprocessor(object):\n",
    "    def __init__(self):\n",
    "        self.spchars = re.compile('\\`|\\~|\\!|\\@|\\#|\\$|\\%|\\^|\\&|\\*|\\(|\\)|\\_|\\+|\\=|\\\\|\\||\\{|\\[|\\]|\\}|\\:|\\;|\\'|\\\"|\\<|\\,|\\>|\\?|\\/|\\.|\\-')\n",
    "    def __call__(self, doc):\n",
    "        return self.spchars.sub('', doc.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(preprocessor=my_preprocessor(),stop_words='english')\n",
    "X = cv.fit_transform(df.all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'data',\n",
      " u'python',\n",
      " u'learning',\n",
      " u'talk',\n",
      " u'using',\n",
      " u'analysis',\n",
      " u'use',\n",
      " u'pandas',\n",
      " u'models',\n",
      " u'machine',\n",
      " u'tools',\n",
      " u'ill',\n",
      " u'analytics',\n",
      " u'code',\n",
      " u'science',\n",
      " u'discuss',\n",
      " u'new',\n",
      " u'model',\n",
      " u'like',\n",
      " u'used',\n",
      " u'tutorial',\n",
      " u'time',\n",
      " u'techniques',\n",
      " u'spark',\n",
      " u'work',\n",
      " u'library',\n",
      " u'methods',\n",
      " u'coming',\n",
      " u'software',\n",
      " u'based',\n",
      " u'algorithm',\n",
      " u'algorithms',\n",
      " u'building',\n",
      " u'soon',\n",
      " u'libraries',\n",
      " u'applications',\n",
      " u'statistical',\n",
      " u'performance',\n",
      " u'space',\n",
      " u'deep']\n"
     ]
    }
   ],
   "source": [
    "fn = np.array(cv.get_feature_names())\n",
    "common_words = fn[np.argsort(X.toarray().sum(axis=0))[-40:]]\n",
    "pprint(list(common_words[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'talk', 'NN'),\n",
       " (u'analysis', 'NN'),\n",
       " (u'use', 'NN'),\n",
       " (u'machine', 'NN'),\n",
       " (u'science', 'NN'),\n",
       " (u'model', 'NN'),\n",
       " (u'time', 'NN'),\n",
       " (u'work', 'NN'),\n",
       " (u'software', 'NN'),\n",
       " (u'performance', 'NN'),\n",
       " (u'space', 'NN'),\n",
       " (u'deep', 'NN')]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter(lambda x: x[1] == 'NN', pos_tag(common_words[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
